# Main Hydra configuration file for Boolean NCA training

# Default parameters for experiment
defaults:
  - model: self_attention  # Default model (gnn or self_attention)
  - _self_  # For self-overrides

# Random seed for reproducibility
seed: 0
test_seed: 42

# Wandb configuration
wandb:
  enabled: true
  project: "boolean-nca-cc"
  entity: m2snn  # Your wandb username or team name
  run_name: null  # Auto-generated if null
  tags: ["boolean-circuit", "nca", "meta-learning"]


# Circuit configuration
circuit:
  task: "binary_multiply"
  input_bits: 8
  output_bits: 8
  arity: 4
  num_layers: 3
  layer_sizes: null  # Auto-generate if null 

# Training parameters
training:
  # WIRING MODE : "random" or "fixed"
  wiring_mode: "random"  # Options: "fixed" or "random"
  # INNER LOOP
  n_message_steps: 30  # Number of message passing steps per training iteration
  # LOSS TYPE
  loss_type: "l4"  # Options: "l4", "l2", or "bce"
  random_loss_step: true  # Use random message passing step for loss computation
  use_beta_loss_step: true  # Use beta distribution for random loss step (varies from early to late steps through training)
  # MESSAGE STEPS CURRICULUM LEARNING
  message_steps_schedule:
    enabled: false  # Whether to use message steps scheduling (curriculum learning)
    type: "linear"  # Options: "constant", "linear", "exponential", "step"
    initial_steps: 2   # Start with fewer steps for easier gradients
    final_steps: ${training.n_message_steps}    # End with more steps for final performance
    constant_product: null  # Constant product of (meta_batch_size * n_message_steps) to adjust batch size (if null, uses base_batch_size allways)
    transition_epochs: null  # For linear schedule (if null, uses total training epochs)
  # BATCH SIZE
  meta_batch_size: 32
  # OPTIMIZATION
  learning_rate: 1e-4
  weight_decay: 1e-5
  # LEARNING RATE SCHEDULER
  lr_scheduler: "linear_warmup"  # Options: "constant", "exponential", "cosine", "linear_warmup"
  lr_scheduler_params:
    warmup_steps_factor: 50
    lr_warmup_start: 1e-6
    exponent: 1
    alpha: 1e-2 # final lr = 1% of initial lr
  # NUMBER OF EPOCHS
  epochs: null
  epochs_power_of_2: 18 # 2^n
  # USE SCAN
  use_scan: true


# Pool configuration for meta-learning
pool:
  size: 1024
  reset_fraction: 0.05
  reset_interval: 16 # using reset_interval_schedule instead
  reset_strategy: "combined"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
  combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined reset strategy

  # Reset interval scheduling (frequent early -> rare late)
  reset_interval_schedule:
    enabled: false
    type: "linear"  # Options: "constant", "linear", "exponential"
    initial_interval: 32     # Start with frequent resets (every 10 epochs)
    final_interval: 1024      # End with rare resets (every 500 epochs) 
    # For exponential decay
    decay_rate: 0.95         # Exponential decay factor per epoch
    # For linear decay  
    transition_epochs: null  # If null, uses total training epochs

  # Gate knockout ("hard" LUT zeroing - resets globals)
  gate_knockout:
    active: false
    fraction: 0.1
    interval: 64
    strategy: "uniform"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
    damage_prob: 0.05  # Probability of zeroing an entire LUT in selected circuits
    combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined strategy

  # Soft LUT damage ("soft" LUT zeroing - preserves globals)
  soft_lut_damage:
    active: false
    fraction: 0.01
    interval: 64
    strategy: "uniform"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
    damage_prob: 0.1 # Probability of zeroing an entire LUT in selected circuits
    combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined strategy

# Backpropagation baseline
backprop:
  enabled: false
  epochs: 100 
  learning_rate: 1
  weight_decay: 1e-1
  optimizer: "adamw"
  beta1: 0.8
  beta2: 0.8
  
# Evaluation parameters
eval:
  inner_steps: 100  # Number of message passing steps for inner loop evaluation
  # Periodic evaluation during training
  periodic:
    enabled: true  # Whether to run periodic evaluation during training
    interval: 256  # Evaluate every N epochs
    log_stepwise: false  # Whether to log detailed step-by-step metrics to wandb
    batch_size: 64  # Batch size for random wiring evaluation
    log_pool_scatter: true

# Checkpointing
checkpoint:
  enabled: true
  interval: 1024  # Save checkpoints every N epochs
  save_best: true  # Whether to track and save the best model
  best_metric: "hard_accuracy"  # Metric to use for determining best model (loss, hard_loss, accuracy, hard_accuracy)
  save_stable_states: false  # Whether to save a checkpoint before potential NaN losses

# Logging configuration
logging:
  log_interval: 16  # Log metrics every N epochs
  level: "INFO" 