# Main Hydra configuration file for Boolean NCA training

# Default parameters for experiment
defaults:
  - model: self_attention  # Default model (gnn or self_attention)
  - _self_  # For self-overrides

# Random seed for reproducibility
seed: 0
test_seed: 42

# Output configuration
output:
  dir: null  # Output directory (null means use current working directory)

# Wandb configuration
wandb:
  enabled: true
  project: "boolean-nca-cc"
  entity: m2snn  # Your wandb username or team name
  run_name: null  # Auto-generated if null
  tags: ["boolean-circuit", "nca", "meta-learning"]
  group: null

# Circuit configuration
circuit:
  task: "reverse"
  input_bits: 8
  output_bits: 8
  arity: 4
  num_layers: 3
  width_factor: 2
  layer_sizes: null  # Auto-generate if null 
  circuit_hidden_dim: 32
  text: "Hello NCAs!"

# Graph configuration
graph:
  neighboring_connections: false  # Enable neighboring connections between adjacent gates in the same layer
  bidirectional_edges: true  # Create edges in both forward and backward directions

# Training parameters
training:
  # WIRING MODE : "random" or "fixed"
  wiring_mode: "random"  # Options: "fixed", "genetic", or "random"
  initial_diversity: 256
  # INNER LOOP
  n_message_steps: 5  # Number of message passing steps per training iteration
  # LOSS TYPE
  loss_type: "l4"  # Options: "l4", "l2", or "bce"
  random_loss_step: false  # Use random message passing step for loss computation
  use_beta_loss_step: false  # Use beta distribution for random loss step (varies from early to late steps through training)
  # BATCH SIZE
  meta_batch_size: 256
  batch_chunk_size: null  # Sequential batch processing chunk size (null means use meta_batch_size)
  # OPTIMIZATION
  learning_rate: 1e-4
  weight_decay: 1e-5
  # LEARNING RATE SCHEDULER
  lr_scheduler: "linear_warmup"  # Options: "constant", "exponential", "cosine", "linear_warmup"
  lr_scheduler_params:
    warmup_steps_factor: 50
    lr_warmup_start: 1e-6
    exponent: 1
    alpha: 1e-1 # final lr = 1% of initial lr
  # NUMBER OF EPOCHS
  epochs: null
  epochs_power_of_2: 18 # 2^n
  # USE SCAN
  use_scan: true

# Pool configuration for meta-learning
pool:
  size: 4096
  reset_fraction: null
  reset_interval: 128
  reset_strategy: "uniform"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
  combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined reset strategy
  
  # Automatic parameter computation based on target expected updates
  # If expected_updates is specified and some parameters above are null, 
  # the system will automatically compute the missing parameter(s)
  expected_updates: 96  # Target expected updates per circuit (e.g., 50.0)
  
  # Genetic mutation parameters
  mutation_rate: 1e-3 # For genetic mutation rate
  n_swaps_per_layer: null # For genetic mutation number of swaps per layer
  initial_diversity: ${training.initial_diversity} # For genetic mutation number of initial wires

  # Wire shuffling
  wire_shuffling:
    active: false
    fraction: 0.125
    interval: ${pool.reset_interval}
    strategy: "uniform"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
  
  # Gate knockout
  gate_knockout:
    active: false
    fraction: 0.1
    interval: 64
    strategy: "uniform"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
    damage_prob: 0.05  # Probability of zeroing an entire LUT in selected circuits
    combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined strategy

  # Soft LUT damage
  soft_lut_damage:
    active: false
    fraction: 0.01
    interval: 64
    strategy: "uniform"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
    damage_prob: 0.1 # Probability of zeroing an entire LUT in selected circuits
    combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined strategy

# Backpropagation baseline
backprop:
  enabled: false
  epochs: 100 
  learning_rate: 1
  weight_decay: 1e-1
  optimizer: "adamw"
  beta1: 0.8
  beta2: 0.8
  
# Evaluation parameters
eval:
  inner_steps: 100  # Number of message passing steps for inner loop evaluation
  # Periodic evaluation during training
  log_stepwise: false  # Whether to log detailed step-by-step metrics to wandb
  batch_size: 256  # Batch size for random wiring evaluation
  log_pool_scatter: true
  periodic:
    enabled: true  # Whether to run periodic evaluation during training
    interval: 1024  # Evaluate every N epochs

# Early stopping based on accuracy
stop_accuracy:
  enabled: true  # Whether to enable early stopping based on accuracy
  threshold: 0.995  # Accuracy threshold to trigger early stopping
  metric: "hard_accuracy"  # Which accuracy metric to use ("accuracy" or "hard_accuracy")
  source: "training"  # Source of the metric ("training" or "eval")
  patience: 10  # Number of epochs to wait after reaching threshold before stopping
  min_epochs: 1012 # Minimum number of epochs before early stopping can occur (eval every 512 epochs - 10 epochs)

# Checkpointing
checkpoint:
  enabled: true
  interval: 1031  # Save checkpoints every N epochs (prime number to avoid rest pool before)
  save_best: true
  # Multi-metric best model tracking - specify which metrics to track and save
  track_best_metrics:
    enabled: true  # Whether to enable multi-metric tracking (in addition to the single best_metric above)
    metrics:
      # Format: "source_metric" where source is 'training', 'eval_in', or 'eval_out'
      # and metric is 'hard_accuracy', 'accuracy', 'hard_loss', or 'loss'
      - "eval_in_hard_accuracy"    # In-distribution hard accuracy (most important)
      - "eval_out_hard_accuracy"   # Out-of-distribution hard accuracy (generalization)
      # - "training_hard_accuracy"   # Training hard accuracy (uncomment if needed)
      # - "eval_in_hard_loss"        # In-distribution hard loss (uncomment if needed)
      # - "eval_out_hard_loss"       # Out-of-distribution hard loss (uncomment if needed)

# Logging configuration
logging:
  log_interval: 16  # Log metrics every N epochs
  level: "INFO" 
