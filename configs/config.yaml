# Main Hydra configuration file for Boolean NCA training

# Default parameters for experiment
defaults:
  - model: gnn  # Default model (gnn or self_attention)
  - _self_  # For self-overrides

hydra:
  mode: RUN
#  sweeper:
#    params:
#      circuit.input_bits: 2,3,4
#      circuit.output_bits: 2,3,4
#      circuit.num_layers: 2,3,4
#      training.wiring_mode: fixed,random
#      training.epochs_power_of_2: 8,16
#      training.n_message_steps: 1,10,25
#      pool.gate_knockout.active: true,false
#      pool.soft_lut_damage.active: true,false
#      backprop.enabled: true

# Random seed for reproducibility
seed: 0
test_seed: 42

# Wandb configuration
wandb:
  enabled: true
  project: "boolean-nca-cc"
  entity: m2snn  # Your wandb username or team name
  run_name: null  # Auto-generated if null
  tags: ["boolean-circuit", "nca", "meta-learning"]


# Circuit configuration
circuit:
  task: "binary_multiply"
  input_bits: 8
  output_bits: 8
  arity: 4
  num_layers: 5
  layer_sizes: null  # Auto-generate if null 

# Training parameters
training:
  learning_rate: 3e-3
  weight_decay: 1e-5
  epochs: null
  epochs_power_of_2: 13 # 2^n
  loss_type: "l4"  # Options: "l4" or "bce"
  wiring_mode: "fixed"  # Options: "fixed" or "random"
  meta_batch_size: 32
  n_message_steps: 1  # Number of message passing steps per training iteration
  lr_scheduler: "linear_warmup"  # Options: "constant", "exponential", "cosine", "linear_warmup"
  
# Pool configuration for meta-learning
pool:
  size: 1024
  reset_fraction: 0.01
  reset_interval: 32  # 2^5
  reset_strategy: "combined"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
  combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined reset strategy

  # Gate knockout ("hard" LUT zeroing - resets globals)
  gate_knockout:
    active: true
    fraction: 0.05
    interval: 64 
    strategy: "uniform"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
    damage_prob: 0.1  # Probability of zeroing an entire LUT in selected circuits
    combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined strategy

  # Soft LUT damage ("soft" LUT zeroing - preserves globals)
  soft_lut_damage:
    active: true
    fraction: 0.05
    interval: 64
    strategy: "uniform"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
    damage_prob: 0.1 # Probability of zeroing an entire LUT in selected circuits
    combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined strategy

# Backpropagation baseline
backprop:
  enabled: true
  epochs: 100 
  learning_rate: 1
  weight_decay: 1e-1
  optimizer: "adamw"
  beta1: 0.8
  beta2: 0.8
  
# Evaluation parameters
eval:
  inner_steps: 100  # Number of message passing steps for inner loop evaluation
  
# Checkpointing
checkpoint:
  enabled: true
  interval: 1024  # Save checkpoints every N epochs
  save_best: true  # Whether to track and save the best model
  best_metric: "hard_accuracy"  # Metric to use for determining best model (loss, hard_loss, accuracy, hard_accuracy)
  save_stable_states: true  # Whether to save a checkpoint before potential NaN losses

# Logging configuration
logging:
  log_interval: 10  # Log metrics every N epochs
  level: "INFO" 

# execution_environment:
#   target_gpu_ids: "1"  # Specify intended GPU ID(s), e.g., "0", "0,1" 

# gpu_id: 2

# latest: shuffled wires, soft damage
# second latest: shuffled wires, no kind of damage
# third latest: shuffled and both damages

# MORNING RUNS
# 1. shuffled wires, no damage, reset pool 0.01
# 2. shuffled wires, soft damage, reset pool 0.01 for 2^18 epochs
# 3. shuffled wires, soft damage, reset pool 0.01 for 2^18 epochs???
# 4. SANITY CHECK: fixed wires, no damage, reset pool 0.01 for 2^18 epochs
# 5. fixed wires, both damage, reset pool 0.01 for 2^19 epochs