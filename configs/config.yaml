# Main Hydra configuration file for Boolean NCA training

# Default parameters for experiment
defaults:
  - model: self_attention  # Default model (gnn or self_attention)
  - _self_  # For self-overrides

# Random seed for reproducibility
seed: ${test_seed}
test_seed: 33
damage_seed: 481  # Independent seed for damage pattern generation

# Output configuration
output:
  dir: null  # Output directory (null means use current working directory)

# Wandb configuration
wandb:
  enabled: true
  project: "boolean-nca-cc"
  entity: marcello-barylli-growai  # Your wandb username or team name
  run_name: null  # Auto-generated if null
  tags: ["boolean-circuit", "nca", "meta-learning"]
  group: null

# Circuit configuration
circuit:
  task: "binary_multiply"
  input_bits: 8
  output_bits: 8
  arity: 4
  num_layers: 3
  layer_sizes: null  # Auto-generate if null 
  circuit_hidden_dim: 64

# Training parameters
training:
  # TRAINING MODE
  training_mode: "repair"  # Options: "growth" (nops -> configured) or "repair" (configured -> damaged -> recovery)
  # INNER LOOP
  n_message_steps: 5  # Number of message passing steps per training iteration
  # MESSAGE PASSING GRAPH AUGMENTATION
  layer_neighbors: false  # Allow attention between adjacent gates within a layer
  # ATTENTION MASKING POLICY
  damage_emission: false  # If true, knocked-out nodes cannot receive but can still send
  # LOSS TYPE
  loss_type: "l4"  # Options: "l4", "l2", or "bce"
  # BATCH SIZE
  meta_batch_size: 64
  # OPTIMIZATION
  learning_rate: 2e-4
  weight_decay: 1e-5
  # LEARNING RATE SCHEDULER
  lr_scheduler: "linear_warmup"  # Options: "constant", "exponential", "cosine", "linear_warmup"
  lr_scheduler_params:
    warmup_steps_factor: 50
    lr_warmup_start: 1e-6
    exponent: 1
    alpha: 1e-1 # final lr = 1% of initial lr
  # NUMBER OF EPOCHS
  epochs: 8192 # 32 #6144
  # USE SCAN
  use_scan: false

pool:
  size: 1024
  reset_fraction: 0.15
  reset_interval: 128
  reset_strategy: "combined"  # Options: "uniform", "steps_biased", "loss_biased", "combined"
  combined_weights: [0.5, 0.5] # Weights for [loss, steps] in combined reset strategy

  # Sample-based damage (applied post-step, independent of resets)
  damage_pool_enabled: false
  damage_pool_interval: 128
  damage_pool_fraction: 0.15
  damage_strategy: "loss_biased"
  damage_combined_weights: [0.5, 0.5]
  damage_mode: "greedy"  # Options: "shotgun" (random), "strip" (localized), or "greedy" (ordered selection)
  # Number of gates to knock out when generating fresh patterns (ignored if vocabulary is used)
  damage_prob: 20
  # Damage-specific vocabulary size (decoupled from legacy persistent_knockout)
  damage_knockout_diversity: 1 
  # Pool update step filtering for damage selection (prevents damage to over-optimized circuits)
  damage_min_pool_updates: 0    # Minimum pool updates before circuit can be damaged
  damage_max_pool_updates: 100   # Maximum pool updates before circuit becomes too fragile for damage
  # Number of message passing steps for damage evaluation (baseline and recovery trajectories)
  # This should typically match or be close to the training n_message_steps for realistic recovery assessment
  damage_eval_steps: ${eval.periodic_eval_inner_steps}
  
  # Greedy damage mode configuration
  greedy_ordered_indices: [48, 17, 52, 146, 154, 30, 35, 33, 68, 145, 12, 99, 139, 46, 111, 144, 57, 153, 10, 64]


eval:
  # Periodic evaluation parameters
  periodic_eval_enabled: true
  periodic_eval_inner_steps: 20
  periodic_eval_interval: 256 # 16
  periodic_eval_test_seed: ${test_seed}
  periodic_eval_log_stepwise: true
  periodic_eval_batch_size: ${pool.damage_knockout_diversity}  # Batch size for knockout evaluation (should match knockout_diversity)
  # SEU evaluation configuration
  seu:
    enabled: true
    fraction: ${pool.damage_pool_fraction}
    flips_per_gate: 16  # 2**arity = 2**4 = 16
    # SEU-specific selection controls (decoupled from knockout eval)
    gates_per_circuit: ${pool.damage_prob}
    selection_strategy: ${pool.damage_strategy}  # uniform|steps_biased|loss_biased|combined
    gate_selection: "greedy"  # random|greedy
    min_pool_updates: ${pool.damage_min_pool_updates}
    max_pool_updates: ${pool.damage_max_pool_updates}
    greedy_ordered_indices: ${pool.greedy_ordered_indices}
    log_stepwise: ${eval.periodic_eval_log_stepwise}
  
  # Knockout evaluation
  knockout_eval:
    enabled: false
    damage_prob: ${pool.damage_prob}
  
  # Hamming distance analysis
  hamming_analysis_dir: "results/hamming_analysis_mbs${training.meta_batch_size}_div${pool.damage_knockout_diversity}_dam${pool.damage_prob}"  # Directory for hamming analysis plots and CSVs
  
  # Final BP vs SA comparison evaluation
  final_eval_enabled: true
  final_eval_inner_steps: ${eval.periodic_eval_inner_steps}
  # final_eval_seed: 999

# Logging configuration
logging:
  log_interval: 16  # Log metrics every N epochs
  level: "INFO" 

# Checkpoint configuration
checkpoint:
  enabled: false
  interval: 1031  # prime number 
  save_best: true  # Save best model based on metric
  best_metric: "hard_accuracy"  # Metric to track for best model
  best_metric_source: "eval_ko_in"  # Source for best metric: "training", "eval", or "eval_ko_in"
  save_stable_states: false  # Save last stable state before NaN loss

# Early stopping configuration
early_stop:
  enabled: true  # Enable early stopping based on accuracy threshold
  threshold: 0.995  # Accuracy threshold to trigger early stopping
  metric: "hard_accuracy"  # Metric to monitor: "loss", "hard_loss", "accuracy", "hard_accuracy"
  source: "eval_seu_steps"  # Source for early stopping: "training", "eval", "eval_ko_in", or "eval_seu_steps"
  patience: 10  # Number of epochs above threshold before stopping
  min_epochs: 1042  # Minimum epochs before early stopping can trigger

# Backpropagation baseline configuration
backprop:
  enabled: true
  epochs: 200 
  learning_rate: 1
  weight_decay: 1e-1
  optimizer: "adamw"
  beta1: 0.8
  beta2: 0.8
  
  # Parallel training configuration
  parallel: true  # Enable parallel training for knockout patterns
  batch_size: null  # Batch size for parallel training (null = all patterns at once, smaller = less memory)
  
  # Knockout vocabulary configuration for backprop training
  knockout_vocabulary:
    enabled: ${pool.damage_pool_enabled}  # Set to true to enable knockout patterns in backprop
    size: ${pool.damage_knockout_diversity}  # Number of knockout patterns to test
    damage_prob: ${pool.damage_prob}  # Number of gates to knock out per pattern 