# @package _global_
# Default Adam optimizer configuration

optimizer:
  name: "adam"
  learning_rate: 1.0
  b1: 0.8
  b2: 0.8