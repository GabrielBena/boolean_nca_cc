# @package _global_
# AdamW optimizer with weight decay

optimizer:
  name: "adamw"
  learning_rate: 0.01
  b1: 0.9
  b2: 0.999
  weight_decay: 1e-2
