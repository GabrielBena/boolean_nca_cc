# -*- coding: utf-8 -*-
"""Copy of random_boolean_nets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M5rxKESSgCGzKzSAYHNtIj26L854vnPW
"""

# @title import & utils
from collections import namedtuple
from functools import partial
import numpy as np
import matplotlib.pylab as pl
import PIL.Image, PIL.ImageFont, PIL.ImageDraw
from types import SimpleNamespace as Bunch
from IPython.display import clear_output, Image
import plotly.graph_objects as go
from ipywidgets import interact

pl.style.use("dark_background")
from IPython.display import display


def imshow(a):
    display(np2pil(a))


def np2pil(a):
    a = np.asarray(a)
    if a.dtype.kind == "f":
        a = np.uint8(a.clip(0, 1) * 255)
    return PIL.Image.fromarray(a)


def zoom(a, k=2):
    return np.repeat(np.repeat(a, k, 1), k, 0)


def tile2d(a, w=None):
    a = np.asarray(a)
    if w is None:
        w = int(np.ceil(np.sqrt(len(a))))
    th, tw = a.shape[1:3]
    pad = (w - len(a)) % w
    a = np.pad(a, [(0, pad)] + [(0, 0)] * (a.ndim - 1), "constant")
    h = len(a) // w
    a = a.reshape([h, w] + list(a.shape[1:]))
    a = np.rollaxis(a, 2, 1).reshape([th * h, tw * w] + list(a.shape[4:]))
    return a


import jax
import jax.numpy as jp
import optax

input_n, output_n = 8, 8
case_n = 1 << input_n
arity, layer_width, layer_n = 4, 64, 5
layer_sizes = (
    [(input_n, 1)]
    + [(layer_width, arity)] * (layer_n - 1)
    + [(layer_width // 2, arity // 2), (output_n, 1)]
)


def gen_wires(key, in_n, out_n, arity, group_size):
    edge_n = out_n * arity // group_size
    n = max(in_n, edge_n)
    return jax.random.permutation(key, n)[:edge_n].reshape(arity, -1) % in_n


def make_nops(gate_n, arity, group_size, nop_scale=3.0):
    I = jp.arange(1 << arity)
    bits = (I >> I[:arity, None]) & 1
    luts = bits[jp.arange(gate_n) % arity]
    logits = (2.0 * luts - 1.0) * nop_scale
    return logits.reshape(gate_n // group_size, group_size, -1)


@jax.jit
def run_layer(lut, inputs):
    # lut:[group_n, group_size, 1<<arity], [arity, ... , group_n]
    for x in inputs:
        x = x[..., None, None]
        lut = (1.0 - x) * lut[..., ::2] + x * lut[..., 1::2]
    # [..., group_n, group_size, 1]
    return lut.reshape(*lut.shape[:-3] + (-1,))


def run_circuit(logits, wires, x, hard=False):
    acts = [x]
    for ws, lgt in zip(wires, logits):
        luts = jax.nn.sigmoid(lgt)
        if hard:
            luts = jp.round(luts)
        x = run_layer(luts, [x[..., w] for w in ws])
        acts.append(x)
    return acts


def gen_circuit(key, layer_sizes, arity=4):
    in_n = layer_sizes[0][0]
    all_wires, all_logits = [], []
    for out_n, group_size in layer_sizes[1:]:
        wires = gen_wires(key, in_n, out_n, arity, group_size)
        logits = make_nops(out_n, arity, group_size)
        _, key = jax.random.split(key)
        in_n = out_n
        all_wires.append(wires)
        all_logits.append(logits)
    return all_wires, all_logits


def res2loss(
    res, power=2
):  # power=4 seems to converge better on some problems, but 2 is also ok
    return (res**power).sum()


def loss_f(logits, wires, x, y0):
    act = run_circuit(logits, wires, x)
    hard_act = run_circuit(logits, wires, x, hard=True)
    loss = res2loss(act[-1] - y0)
    hard_loss = res2loss(hard_act[-1] - y0)
    return loss, dict(act=act, loss=loss, hard_loss=hard_loss)


grad_loss_f = jax.jit(jax.value_and_grad(loss_f, has_aux=True))

key = jax.random.PRNGKey(42)
wires, logits0 = gen_circuit(key, layer_sizes)

opt = optax.adamw(2.0, 0.8, 0.8, weight_decay=0.1)

TrainState = namedtuple("TrainState", "params opt_state")
state = TrainState(params=logits0, opt_state=opt.init(logits0))


def unpack(x, bit_n=8):
    return jp.float32((x[..., None] >> np.r_[:bit_n]) & 1)


x = jp.arange(case_n)
y0 = (x & 0xF) * (x >> 4)
x, y0 = unpack(x), unpack(y0)


def train_step(state):
    logits, opt_state = state
    (loss, aux), grad = grad_loss_f(logits, wires, x, y0)
    upd, opt_state = opt.update(grad, opt_state, logits)
    logits = optax.apply_updates(logits, upd)
    return aux, TrainState(logits, opt_state)


loss_log, hard_log = [], []
print("inputs")
imshow(zoom(x.T, 4))
print("target outputs")
imshow(zoom(y0.T, 4))

for i in range(100):
    aux, state = train_step(state)
    loss = aux["loss"]
    loss_log.append(aux["loss"])
    hard_log.append(aux["hard_loss"])
    if i % 10 == 0:
        print(i, f"soft: {loss:.2f}  hard: {aux['hard_loss'].item()}")

act = run_circuit(state.params, wires, x, hard=True)
vis = np.hstack(act)
imshow(zoom(vis, 2))

pl.plot(loss_log, label="soft loss")
pl.plot(hard_log, label="hard loss")
pl.legend()
pl.grid()
pl.yscale("log")
# how to encode LUTs with input and output embdings

emb_d = 32
input_emb = np.random.normal(size=[arity, emb_d])
M = unpack(np.arange(1 << arity), arity) * 2.0 - 1.0
case_embedings = M @ input_emb
# f(case_emb, output_emb) -> lut
