The overview of this codebase is in docs/design_doc_ko_selfatt.md

Once the testing suite is concluded, we need to rework how knockouts are introduced into the training flow / pool. Currently, knockouts are introduced as part of the periodic pool reset, therefore knocked out circuits will always start as NOPs. This means that the network will only ever learn to optimise knocked out circuits starting from NOPs.

An alternative design is to instead periodically sample any given circuit present in the pool, apply a knockout mask to it in the same way that currently fresh circuits are knocked out, and to then proceed as usual with pool optimisation. This means that knockouts are introduced randomly to any kinds of circuits along their optimisation trajctory, simulating damage during lifetime.

The question remains as to what to do with the gates that have been removed from the circuits that are knocked out, as their LUTs will be partially optimised and functional. Our options are:
- stuck-at-faults: reset their LUT fields to all 0 after the gate is knocked out. SInce it is also removed from message passing (masked attention), the fields will remain at 0 and not be updated. Perhaps all of the hiddenchannels could be zeroed, but this is a variable design choice to vary and compare.
- gate-removal: If a node is removed from the self-attention process via the attention mask (as is currently implemented), the corresponding gate should also be removed from the circuit, thereby no longer participating in circuit function. Perhaps we could look at how the gate_mask in boolean_nca_cc/circuits/model.py works and leverage this. Or there could be another way of removing gates form the circutis themselves

For deciding which of the above to implement, we must weigh which one requires less invasive changes to the existing program, but also which one seems more feasible to be learned by the network. introducing knockouts during pool optimisation could pose a significant challenge to the learning process.

In any case, the actual evaluation we care about is on circuits that were not knocked out (their LUTs are configured for good hard_accuracy), then the circuit with those already configured LUTs is knocked out, then the self-attention GNN has to reconfigure the remaining gates to recover the circuit function. I believe that this is already done if we implement the above knockout design choices, since at some point the opol will be saturated with circuits that are fully functinoal, so the sampling-based approach will provide examples of knockouts on functional circuits. Then we just have to match the eval function to not create a fresh circuit using gen_circuit, applying knockout and evaluating on N message steps, but to instead sample any circuit from the pool, apply the knock out, and then evaluate after N message passing steps. Therefore, the only difference is the source of circuit for evaluation.

Perhaps we first have to make a decision between the following:
- First implement the gate removal / stuck at faults to the circuits, before changing the overall strategy from reset-based to sampling-based. This would mean making the network learn to optimise from fresh NOPs, but with actual circuit damage
- Implement the gate removal alongside the sampling strategy. It makes little sense to implement only the sampling strategy without affecting the gates themselves, as this will leave them in a partially optimised state when their circuit is sampled for knockout.

Given all the considerations above, we need to plan a course of iterative changes to be made and tested at each stage.